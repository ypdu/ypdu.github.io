---
title: "AI Safety and Alignment - Winter 2025"
collection: courses
type: "Graduate Course"
permalink: /courses/ai-safety-winter-2025/
venue: "Saarland University"
date: 2025-01-01
location: "Saarbr√ºcken, Germany"
---

## Course Overview

This course provides an introduction to AI Safety and Alignment, covering both theoretical foundations and practical approaches to building safe and reliable AI systems.

### Course Information
- **Instructor**: Prof. Alexander Koller, Yupei Du
- **Time**: Mondays & Wednesdays, 10:00-11:30 AM
- **Location**: Building E1.1, Room 1.02
- **Prerequisites**: Machine Learning, Neural Networks, Python programming

## Learning Objectives

By the end of this course, students will be able to:

1. Understand fundamental concepts in AI safety and alignment
2. Identify potential risks and failure modes in AI systems
3. Apply safety techniques to improve AI system reliability
4. Evaluate the safety properties of AI models
5. Design safer AI systems for real-world deployment

## Course Schedule

### Week 1-2: Foundations
- Introduction to AI Safety
- Types of AI risks and failure modes
- The alignment problem

### Week 3-4: Robustness and Reliability  
- Adversarial examples and defenses
- Distribution shift and domain adaptation
- Model uncertainty and calibration

### Week 5-6: Interpretability and Explainability
- Model interpretability techniques
- Attribution methods
- Mechanistic interpretability

### Week 7-8: Value Alignment
- Reward modeling and learning from human feedback
- Constitutional AI approaches
- Multi-objective optimization

### Week 9-10: Evaluation and Testing
- Safety evaluation methodologies
- Red teaming and stress testing
- Formal verification approaches

### Week 11-12: Advanced Topics
- AI governance and policy
- Long-term safety considerations
- Current research frontiers

## Assignments and Evaluation

### Assignment Structure
- **4 Problem Sets** (40%): Theoretical and practical exercises
- **Mid-term Project** (25%): Implement and evaluate a safety technique
- **Final Project** (25%): Original research on an AI safety topic
- **Participation** (10%): Class discussion and presentations

### Key Dates
- **Mid-term Project Due**: March 15, 2025
- **Final Project Presentations**: April 20-22, 2025
- **Final Project Report Due**: April 30, 2025

## Resources

### Required Reading
- Papers will be assigned weekly (available on course website)
- Selected chapters from "Human Compatible" by Stuart Russell

### Recommended Resources
- [AI Safety Gridworlds](https://github.com/deepmind/ai-safety-gridworlds)
- [Anthropic's Constitutional AI paper](https://arxiv.org/abs/2212.08073)
- [MIRI Research](https://intelligence.org/research/)

### Programming Environment
- Python 3.8+
- PyTorch or TensorFlow
- Jupyter Notebooks
- Access to computing resources (provided)

## Course Materials

### Lecture Slides
- Week 1: [Introduction to AI Safety](lectures/week1-intro.pdf)
- Week 2: [Risk Taxonomy](lectures/week2-risks.pdf)
- *More slides will be posted weekly*

### Code Examples
- [Basic robustness evaluation](code/robustness_demo.py)
- [Attribution visualization](code/attribution_demo.ipynb)
- [Uncertainty quantification](code/uncertainty_demo.py)

## Contact and Office Hours

### Instructor Office Hours
- **Prof. Koller**: Tuesdays 2-3 PM, Building E1.1, Room 2.15
- **Yupei Du**: Fridays 1-2 PM, Building E1.1, Room 3.08

### Course Communication
- **Moodle**: Main course page and assignments
- **Discord**: Quick questions and peer discussion
- **Email**: For private matters only

## Assessment Rubric

### Project Evaluation Criteria
- **Technical Quality** (40%): Correctness and sophistication of approach
- **Novelty** (20%): Originality of ideas and methods
- **Evaluation** (20%): Thorough experimental validation  
- **Presentation** (20%): Clear communication of results

### Late Policy
- Problem sets: 20% penalty per day late
- Projects: Must be submitted on time (no extensions except for documented emergencies)

## Academic Integrity

All work must be your own unless explicitly stated as group work. You may discuss concepts with classmates, but implementations and write-ups must be individual. Proper citation is required for all external sources.

---

*This course page will be updated throughout the semester. Please check regularly for announcements and new materials.*

![AI Safety](https://media.giphy.com/media/3oKIPEqDGUULpEU0aQ/giphy.gif)

*Let's build safer AI together!*